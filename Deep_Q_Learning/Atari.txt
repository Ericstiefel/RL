The goal is to train a Deep Q Agent to play atari games.

Documentation Link: https://ale.farama.org/environments/space_invaders/

Motivation behind Deep Q Learning:
There are more possible states in the game of Atari than atoms in the universe, and training an agent to play atari would take far beyond my lifetime.
Therefore, we need a Machine Learning Algorithm that performs like the Q-Table on a continuous plane.
As a result of the global approximation theorem (Quick Summary: As deep enough Neural Net can approximate any general function), we try to make our neural net behave 
similar to how our Q-Table would on such a vast observation space, without actually having to view every possible state.


Observation Space:
Box(0,255,(210,160,3), unit8)
Array with 8bit ranging in [0,255] with observations of size (210,160,3)

Action Space: Discrete(6)
0: None
1: Fire
2: Right
3: Left
4: Right Fire
5: Left Fire

Reward: 
Gain points for killing aliens, with the farthest away resulting in most
Reward of 200+ is completion bound

DATA PRE-PROCESSING
Input Data on Atari (210x160x3)
Remove color(greyscale) (210x160x3)
Reshape (84x84)
Group 4 consecutive observations as a state (84x84x4)

Q-TABLE
Same as Q-Learning

ACTION SELECTION
epsilon-greedy

NEURAL NET
Convolution
Error: MSE(Q-Table, NNpred)
Optimizer: Gradient Descent

Train on mini-batch from replay buffer 


Process:
(1) Recieve state from environment
(2) Modify state to desired outcome
(3) Choose Action through Epsilon-Greedy 
(4) Perform Action & Recieve (next_state, reward, ..)
(5) Store (state, action, reward, next_state) in replay buffer 
(6) Calculate Q-Value through Bellman Equation
(7) Sample Mini-Batch from replay buffer
(8) Update NN using MSE (Q-Table values & corresponding NN predictions)
